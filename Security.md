# Dataflow security and permissions 

### âœ… Running Dataflow Pipelines

-> Pipelines can run locally (for testing) or on Google Cloud (for scalability) using the Dataflow runner.

-> Dataflow VMs may need upgrades for better performance.

### âœ… Roles & Permissions

-> Dataflow uses IAM roles to control access.

-> Running pipelines requires specific roles and permissions.

-> Accessing pipeline resources (e.g., storage, Pub/Sub, BigQuery) needs proper IAM permissions.

### âœ… Data & Security in Dataflow

-> Handles structured & unstructured data.

-> Ensures data security through encryption, IAM roles, and access controls.

# Upgrade and patch Dataflow VMs

âœ… Dataflow VMs run on Container-Optimized OS, which follows strict security processes.

### âœ… Batch Pipelines

-> Use the latest Dataflow image automatically when a new job starts.

-> No manual maintenance required.

### âœ… Streaming Pipelines

-> Google Cloud sends security alerts if immediate patches are needed.

-> Use --update to restart your job with the latest Dataflow image.
        
    ðŸ“Œ Batch jobs update automatically, while streaming jobs may require manual updates for security patches. ðŸš€

# Security & Permissions for Local Pipelines

### 1ï¸âƒ£ How Local Pipelines Run

-> When you run an Apache Beam pipeline locally, it uses the Google Cloud account set up in the gcloud CLI.

-> This means your local system and Google Cloud account have access to the same files and resources.

### 2ï¸âƒ£ How to Check Your Google Cloud Account

Run the command:

```gcloud config list```

-> This will show the Google Cloud account and project currently in use.

### 3ï¸âƒ£ Where Local Pipelines Can Store Data

-> Locally: Save output files on your machine.

-> Cloud: Write data to Cloud Storage, BigQuery, etc.

### 4ï¸âƒ£ Permissions for Cloud Storage

-> If writing data to Google Cloud resources, the pipeline uses:

    ->  Your Google Cloud account credentials

    -> The Google Cloud project set as default in gcloud CLI

### key points:

    âœ” Local pipelines use your Google Cloud account for authentication.
    
    âœ” They can store data locally or in Google Cloud.

    âœ” Writing to cloud resources requires authentication via the gcloud CLI. ðŸš€
# Security & Permissions for Dataflow Pipelines on Google Cloud

1ï¸âƒ£ Service Accounts Used by Dataflow

### âœ… Dataflow Service Account (also called Dataflow Service Agent)

    -> Used during job creation to check project quota and create worker VMs.

    -> Also manages the job execution.

### âœ… Worker Service Account

    ->  Used by worker instances to access input/output resources.
  
    -> By default, it uses the Compute Engine default service account.

    -> Best practice: Use a user-managed service account instead of the default one.

### 2ï¸âƒ£ Required IAM Roles & Permissions

ðŸ”¹ To impersonate the service account, your account needs:

   ``` iam.serviceAccounts.actAs role.```
   
ðŸ”¹ Additional permissions for running pipelines:

  ```roles/dataflow.developer``` (needed unless youâ€™re a Project Owner/Editor).

### 3ï¸âƒ£ Best Practices for Security

âœ… Use a User-Managed Service Account for workers instead of the default one.

âœ… Follow the Principle of Least Privilege

    -> Grant only the minimum required permissions for each task.

    -> Create custom roles with the exact permissions needed.

âœ… Limit Scope of Role Assignments
        
    -> Instead of giving bigquery.dataEditor role at the project level, assign it at the table level.
   
âœ… Use a Dedicated Staging Bucket

    -> Create a project-owned bucket for staging Dataflow jobs.

    -> The default bucket permissions allow Dataflow to use it automatically.

# Dataflow Service Account - Key Points

### 1ï¸âƒ£ What is it?

-> A Google-managed service account used by Dataflow to manage resources like VMs.

### 2ï¸âƒ£ Email Format:

``` service-PROJECT_NUMBER@dataflow-service-producer-prod.iam.gserviceaccount.com ```

### 3ï¸âƒ£ How is it created?

-> It is automatically created when you first use a Dataflow Job in your project.

### 4ï¸âƒ£ Role & Permissions:

-> Assigned the Dataflow Service Agent role `(roles/dataflow.serviceAgent)`

-> Has permissions to `start Compute Engine` workers and `manage Dataflow jobs`.

### 5ï¸âƒ£ Do not modify its permissions!

-> Google Cloud services need full access to run Dataflow jobs.

-> Removing permissions may break Dataflowâ€™s ability to create VMs and manage jobs.

### 6ï¸âƒ£ How to check its roles?

-> Use the Google Cloud Console or run:

```
gcloud iam roles describe roles/dataflow.serviceAgent

```
### 7ï¸âƒ£ If permissions are removed?

-> The service account remains in IAM, but Dataflow won't function properly without its required permissions.

# Worker Service Account - Key Points

### 1ï¸âƒ£ What is it?

-> Used by Compute Engine worker VMs to run Apache Beam SDK operations in the cloud.

-> Acts as the identity for all worker VMs, allowing them to access required Google Cloud resources.

### 2ï¸âƒ£ Permissions Required:

`roles/dataflow.worker` â†’ To run jobs.

`roles/dataflow.admin` â†’ To create or examine jobs.

-> Additional permissions required based on the resources accessed (e.g.,` roles/bigquery.dataEditor `for BigQuery).

### 3ï¸âƒ£ Access to Google Cloud Resources:

        -> Cloud Storage buckets
        
        -> BigQuery datasets

        -> Pub/Sub topics and subscriptions

        -> Firestore datasets

### 4ï¸âƒ£ Default Worker Service Account:

-> Uses the Compute Engine default service account:
```
PROJECT_NUMBER-compute@developer.gserviceaccount.com

```
-> This account is created automatically when you enable the Compute Engine API.

### 5ï¸âƒ£ Best Practice: Use a Dedicated Worker Service Account

-> Instead of using the default Compute Engine account, create a custom service account with only the required permissions.

### 6ï¸âƒ£ Security Considerations:

-> By default, the `Compute Engine service account` may have the `"Editor"` role, which grants too many permissions.

-> Disable automatic role grants using:

`iam.automaticIamGrantsForDefaultServiceAccounts` policy constraint.

### 7ï¸âƒ£ Role Management & Least Privilege Principle:

-> If the default account has `Editor` role, replace it with `specific` roles.

-> Use Policy Simulator to analyze permission impact before making changes.

# User-Managed Worker Service Account - Key Steps

### 1ï¸âƒ£ Create a User-Managed Service Account

-> Instead of using the default Compute Engine service account, create a dedicated service account for fine-grained access control.

### 2ï¸âƒ£ Assign Required IAM Roles

`roles/dataflow.worker` â†’ To run Dataflow jobs.

`roles/dataflow.admin` â†’ To create or examine jobs.

-> Additional roles for accessing required resources (e.g., `roles/bigquery.dataViewer` for BigQuery).

### 3ï¸âƒ£ Ensure Access to Staging & Temporary Locations

-> The user-managed service account must have `read/write` access to `staging and temporary` locations used by Dataflow jobs.

### 4ï¸âƒ£ Grant Service Account Permissions

-> Allow the service account to be impersonated:`iam.serviceAccounts.actAs` permission.

-> Ensure Google-managed service accounts have these roles:

-> Service Account Token Creator (`iam.serviceAccountTokenCreator`)

-> Service Account User (`iam.serviceAccountUser`)

### 5ï¸âƒ£ Cross-Project Usage Considerations

-> If the service account and job are in different projects, grant the necessary roles to allow cross-project service account usage.

-> Ensure `iam.disableCrossProjectServiceAccountUsage` constraint is not enforced.

### 6ï¸âƒ£ Specify the Service Account in Your Pipeline Job

Command Line:

```
--serviceAccount=SERVICE_ACCOUNT_NAME@PROJECT_ID.iam.gserviceaccount.com
```
### For Flex Templates:

```
--service-account-email=SERVICE_ACCOUNT_NAME@PROJECT_ID.iam.gserviceaccount.com
```

### 7ï¸âƒ£ Assign Roles via gcloud CLI

-> Grant Service Account User role to your user account:

```
gcloud projects add-iam-policy-binding PROJECT_ID \
  --member="user:EMAIL_ADDRESS" \
  --role=roles/iam.serviceAccountUser
```

-> Assign roles to the worker service account:

```
gcloud projects add-iam-policy-binding PROJECT_ID \
  --member="serviceAccount:PROJECT_NUMBER-compute@developer.gserviceaccount.com" \
  --role=SERVICE_ACCOUNT_ROLE

```
Replace placeholders:

        PROJECT_ID â†’ Your project ID

        EMAIL_ADDRESS â†’ Your user email

        PROJECT_NUMBER â†’ Your project number
        
        SERVICE_ACCOUNT_ROLE â†’ Required roles (e.g.,` roles/dataflow.worker`, `roles/dataflow.admin`)

# Accessing Google Cloud Resources for Apache Beam Pipelines

### 1. Resources That Require Access

        -> Artifact Registry (for storing container images)

        -> Cloud Storage (for reading/writing files)

        -> BigQuery (for querying datasets)
        
        -> Pub/Sub (for message streaming)
        
        -> Firestore (for structured data storage)
        
### 2. Granting Access to the Dataflow Worker Service Account

-> Use the Compute Engine default service account:

```
PROJECT_NUMBER-compute@developer.gserviceaccount.com
```
-> Or configure a user-managed service account.


# 3. Permissions Required for Each Resource

### Artifact Registry â†’ `roles/artifactregistry.writer`
 

-> Grant Artifact Registry Writer role:

```
gcloud projects add-iam-policy-binding PROJECT_ID \
  --member="serviceAccount:PROJECT_NUMBER-compute@developer.gserviceaccount.com" \
  --role="roles/artifactregistry.writer"

```


### Cloud Storage â†’ `roles/storage.objectViewer`, `roles/storage.objectCreator`

-> Grant Storage Object Viewer/Creator roles: (for reading/writing files)

```
gcloud storage buckets add-iam-policy-binding gs://BUCKET_NAME \
  --member="serviceAccount:PROJECT_NUMBER-compute@developer.gserviceaccount.com" \
  --role="roles/storage.objectViewer"

gcloud storage buckets add-iam-policy-binding gs://BUCKET_NAME \
  --member="serviceAccount:PROJECT_NUMBER-compute@developer.gserviceaccount.com" \
  --role="roles/storage.objectCreator"
```
-> List existing buckets

```
gcloud storage buckets list --project=PROJECT_ID
```

### BigQuery â†’ `roles/bigquery.dataEditor`, `roles/bigquery.jobUser`

-> Grant the following roles: (for reading/writing datasets)

        roles/bigquery.dataEditor (modify datasets)
        
        roles/bigquery.jobUser (run queries )

```

gcloud projects add-iam-policy-binding PROJECT_ID \
  --member="serviceAccount:PROJECT_NUMBER-compute@developer.gserviceaccount.com" \
  --role="roles/bigquery.dataEditor"

gcloud projects add-iam-policy-binding PROJECT_ID \
  --member="serviceAccount:PROJECT_NUMBER-compute@developer.gserviceaccount.com" \
  --role="roles/bigquery.jobUser"
```


### Pub/Sub â†’ `roles/pubsub.subscriber`

-> Grant necessary roles based on use case: (for message streaming)

        roles/pubsub.subscriber (consume messages)
        
        roles/pubsub.editor (create subscriptions)
                
        roles/pubsub.viewer (query topic settings)

```
gcloud projects add-iam-policy-binding PROJECT_ID \
  --member="serviceAccount:PROJECT_NUMBER-compute@developer.gserviceaccount.com" \
  --role="roles/pubsub.subscriber"

```

### Firestore â†’ `roles/datastore.viewer`

-> Grant Datastore Viewer role: (Datastore Mode)

```
gcloud projects add-iam-policy-binding PROJECT_ID \
  --member="serviceAccount:PROJECT_NUMBER-compute@developer.gserviceaccount.com" \
  --role="roles/datastore.viewer"
```

-> Ensure Firestore API is enabled:


```
gcloud services enable firestore.googleapis.com
```

### 4. Additional Considerations

-> Assured Workloads: If using compliance controls (e.g., `EU regions`), all resources must be within an Assured Workloads project or folder.

-> Cross-Project Access: If accessing resources in other projects, grant permissions in both projects.

-> VPC Service Controls: Ensure that org policies or VPC security rules do not block access.


### 5. Best Practices

        âœ… Assign least privilege roles to service accounts.

        âœ… Use IAM roles instead of legacy ACLs.

        âœ… Verify access using: `gcloud projects get-iam-policy PROJECT_ID`

        âœ… Enable required Google Cloud APIs before running pipelines.

# Data Access and Security in Dataflow

### 1. Types of Data in Dataflow

End-User Data: Processed by the pipeline (e.g., input from sources, transformations, output to sinks).

Operational Data: Metadata required to manage the pipeline (e.g., job name, job ID, pipeline options).

### 2. Security Mechanisms Applied to:

        -> Pipeline submission

        -> Pipeline evaluation

        -> Access to logs, telemetry, and metrics

        -> Using Dataflow services (Shuffle, Streaming Engine)

### 3. Data Locality

-> Specify a region for Dataflow jobs to ensure data processing occurs in that region.

-> If no region is specified, `us-central1` is the default.

-> Data processing happens only in the specified region, even if sources/sinks are in different locations.

-> Worker VMs execute pipeline logic in specified zones.

### 4. Security in Different Data Stages

âœ… Pipeline Submission

        -> Requires authentication via Google Cloud CLI.
        
        -> IAM roles (Editor/Owner) control access.
        
        -> Submitted via HTTPS for security.
        
âœ… Pipeline Evaluation

        -> Temporary data may be stored in Cloud Storage or worker VMs.

        -> Temporary data is encrypted at rest and deleted after the job ends.

        -> Compute Engine VMs and attached Persistent Disks are deleted upon job completion.
        
        -> Intermediate data might be found in --stagingLocation or --tempLocation in Cloud Storage.

âœ… Logs and Telemetry

        -> Stored in Cloud Logging.

        -> Dataflow adds only warnings and errors to logs.

        -> Telemetry and metrics are encrypted at rest and access is controlled via IAM.

âœ… Data in Dataflow Services

        -> Dataflow Shuffle/Streaming Engine auto-selects the zone when the region is specified.

        -> Data in transit stays within worker VMs and remains in the pipelineâ€™s specified region.

### 5. Best Practices

ðŸ”¹ Use built-in security mechanisms of storage services (e.g., Cloud Storage, BigQuery).

ðŸ”¹ Avoid mixing different trust levels in a single project.

ðŸ”¹ Always specify a region to ensure optimal data processing and security.

ðŸ”¹ Grant IAM roles with the least privilege needed for the pipeline.
